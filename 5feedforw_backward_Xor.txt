import numpy as np

# Sigmoid activation function and its derivative for backpropagation
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_deriv(x):
    return x * (1 - x)

# XOR dataset: inputs and expected outputs
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([[0], [1], [1], [0]])  # Expected XOR outputs

# Set seed for reproducibility of random values
np.random.seed(0)

# Network architecture
input_neurons = 2       # Number of input features
hidden_neurons = 4      # Number of neurons in hidden layer
output_neurons = 1      # Single output (XOR)

# Randomly initialize weights and biases
W1 = np.random.uniform(size=(input_neurons, hidden_neurons))  # Weights from input to hidden layer
b1 = np.zeros((1, hidden_neurons))                            # Bias for hidden layer
W2 = np.random.uniform(size=(hidden_neurons, output_neurons)) # Weights from hidden to output layer
b2 = np.zeros((1, output_neurons))                             # Bias for output layer

# Training parameters
epochs = 10000  # Number of training iterations
lr = 0.1        # Learning rate

# Training loop
for epoch in range(epochs):
    # ---- FORWARD PROPAGATION ----
    hidden_input = np.dot(X, W1) + b1           # Weighted sum at hidden layer
    hidden_output = sigmoid(hidden_input)       # Activation at hidden layer

    final_input = np.dot(hidden_output, W2) + b2  # Weighted sum at output layer
    predicted_output = sigmoid(final_input)       # Activation at output layer

    # ---- BACKPROPAGATION ----
    error = y - predicted_output                                # Error at output
    d_predicted_output = error * sigmoid_deriv(predicted_output)  # Delta at output

    error_hidden_layer = d_predicted_output.dot(W2.T)               # Error at hidden layer
    d_hidden_layer = error_hidden_layer * sigmoid_deriv(hidden_output)  # Delta at hidden layer

    # ---- UPDATE WEIGHTS AND BIASES ----
    W2 += hidden_output.T.dot(d_predicted_output) * lr    # Update output weights
    b2 += np.sum(d_predicted_output, axis=0, keepdims=True) * lr  # Update output bias
    W1 += X.T.dot(d_hidden_layer) * lr                    # Update hidden weights
    b1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr      # Update hidden bias

# Print the final predicted outputs after training
print("Final predictions after training:")
print(predicted_output.round())  # Rounded to 0 or 1 for clarity
